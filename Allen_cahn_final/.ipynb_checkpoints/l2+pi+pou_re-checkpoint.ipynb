{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "126f3ea6-2ea7-4b40-8007-19c09f44b032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as onp\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "import jax.numpy as np\n",
    "from jax import random, grad, vmap, jit\n",
    "from jax.example_libraries import optimizers\n",
    "from jax import config\n",
    "from jax.flatten_util import ravel_pytree\n",
    "from jax.nn import relu, elu\n",
    "import itertools\n",
    "from functools import partial\n",
    "from torch.utils import data\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69b29c91-fc37-4928-a049-81f3b326cde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define MLP\n",
    "def MLP(layers, activation=relu):\n",
    "  ''' Vanilla MLP'''\n",
    "  def init(rng_key):\n",
    "      def init_layer(key, d_in, d_out):\n",
    "          k1, k2 = random.split(key)\n",
    "          glorot_stddev = 1. / np.sqrt((d_in + d_out) / 2.)\n",
    "          W = glorot_stddev * random.normal(k1, (d_in, d_out))\n",
    "          b = np.zeros(d_out)\n",
    "          return W, b\n",
    "      key, *keys = random.split(rng_key, len(layers))\n",
    "      params = list(map(init_layer, keys, layers[:-1], layers[1:]))\n",
    "      return params\n",
    "  def apply(params, inputs):\n",
    "      for W, b in params[:-1]:\n",
    "          outputs = np.dot(inputs, W) + b\n",
    "          inputs = activation(outputs)\n",
    "      W, b = params[-1]\n",
    "      outputs = np.dot(inputs, W) + b\n",
    "      return outputs\n",
    "  return init, apply\n",
    "\n",
    "# Define modified MLP\n",
    "def modified_MLP(layers, activation=relu):\n",
    "  def xavier_init(key, d_in, d_out):\n",
    "      glorot_stddev = 1. / np.sqrt((d_in + d_out) / 2.)\n",
    "      W = glorot_stddev * random.normal(key, (d_in, d_out))\n",
    "      b = np.zeros(d_out)\n",
    "      return W, b\n",
    "    \n",
    "  def init(rng_key):\n",
    "      U1, b1 =  xavier_init(random.PRNGKey(12345), layers[0], layers[1])\n",
    "      U2, b2 =  xavier_init(random.PRNGKey(54321), layers[0], layers[1])\n",
    "      def init_layer(key, d_in, d_out):\n",
    "          k1, k2 = random.split(key)\n",
    "          W, b = xavier_init(k1, d_in, d_out)\n",
    "          return W, b\n",
    "      key, *keys = random.split(rng_key, len(layers))\n",
    "      params = list(map(init_layer, keys, layers[:-1], layers[1:]))\n",
    "      return (params, U1, b1, U2, b2) \n",
    "\n",
    "  def apply(params, inputs):\n",
    "      params, U1, b1, U2, b2 = params\n",
    "      U = activation(np.dot(inputs, U1) + b1)\n",
    "      V = activation(np.dot(inputs, U2) + b2)\n",
    "      for W, b in params[:-1]:\n",
    "          outputs = activation(np.dot(inputs, W) + b)\n",
    "          inputs = np.multiply(outputs, U) + np.multiply(1 - outputs, V) \n",
    "      W, b = params[-1]\n",
    "      outputs = np.dot(inputs, W) + b\n",
    "      return outputs\n",
    "  return init, apply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17378060-e7f0-4229-8894-24307a4c12c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(data.Dataset):\n",
    "    def __init__(self, u, y, s, \n",
    "                 batch_size=64, rng_key=random.PRNGKey(1234)):\n",
    "        'Initialization'\n",
    "        self.u = u\n",
    "        self.y = y\n",
    "        self.s = s\n",
    "        \n",
    "        self.N = u.shape[0]\n",
    "        self.batch_size = batch_size\n",
    "        self.key = rng_key\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        self.key, subkey = random.split(self.key)\n",
    "        inputs, outputs = self.__data_generation(subkey)\n",
    "        return inputs, outputs\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def __data_generation(self, key):\n",
    "        'Generates data containing batch_size samples'\n",
    "        idx = random.choice(key, self.N, (self.batch_size,), replace=False)\n",
    "        s = self.s[idx,:]\n",
    "        y = self.y[idx,:]\n",
    "        u = self.u[idx,:]\n",
    "        # Construct batch\n",
    "        inputs = (u, y)\n",
    "        outputs = s\n",
    "        return inputs, outputs\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd96edee-650c-438b-afcd-8d42209167df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Physics-informed DeepONet model\n",
    "class PI_DeepONet:\n",
    "    def __init__(self, branch_layers, trunk_layers):    \n",
    "        # Network initialization and evaluation functions\n",
    "        self.branch_init, self.branch_apply = modified_MLP(branch_layers, activation=np.tanh)\n",
    "        self.trunk_init, self.trunk_apply = modified_MLP(trunk_layers, activation=np.tanh)\n",
    "\n",
    "        # Initialize\n",
    "        branch_params = self.branch_init(rng_key = random.PRNGKey(1234))\n",
    "        trunk_params = self.trunk_init(rng_key = random.PRNGKey(4321))\n",
    "        params = (branch_params, trunk_params)\n",
    "\n",
    "        # Use optimizers to set optimizer initialization and update functions\n",
    "        self.opt_init, \\\n",
    "        self.opt_update, \\\n",
    "        self.get_params = optimizers.adam(optimizers.exponential_decay(1e-3, \n",
    "                                                                      decay_steps=2000, \n",
    "                                                                      decay_rate=0.9))\n",
    "        self.opt_state = self.opt_init(params)\n",
    "\n",
    "        # Used to restore the trained model parameters\n",
    "        _, self.unravel_params = ravel_pytree(params)\n",
    "\n",
    "        # Logger\n",
    "        self.itercount = itertools.count()\n",
    "        self.loss_log = []\n",
    "        self.loss_ics_log = []\n",
    "        self.loss_bcs_log = []\n",
    "        self.loss_res_log = []\n",
    "        # Define DeepONet architecture\n",
    "        #前向传播\n",
    "    def operator_net(self, params, u, t, x):\n",
    "        branch_params, trunk_params = params\n",
    "        y = np.stack([t, x], axis=-1)  # 修正这里的维度\n",
    "        B = self.branch_apply(branch_params, u)\n",
    "        T = self.trunk_apply(trunk_params, y)\n",
    "        outputs = np.sum(B * T)\n",
    "        return outputs\n",
    "    \n",
    "    # Define ds/dx\n",
    "    #定义一阶导数，用于边界损失和物理残差的计算。\n",
    "    def s_x_net(self, params, u, t, x):\n",
    "         s_x = grad(self.operator_net, argnums=3)(params, u, t, x)#grad 是 JAX 库中的自动微分函数#grad(f, argnums=n) 返回函数 f 对第 n 个参数的导数，是自动微分工具，常用于物理方程残差和神经网络训练。\n",
    "         return s_x\n",
    "    \n",
    "    # Define PDE residual   \n",
    "    # 用于边界损失和物理残差的计算。     \n",
    "    def residual_net(self, params, u, t, x):\n",
    "        s = self.operator_net(params, u, t, x)\n",
    "        s_t = grad(self.operator_net, argnums=2)(params, u, t, x)\n",
    "        s_x = grad(self.operator_net, argnums=3)(params, u, t, x)\n",
    "        s_xx= grad(grad(self.operator_net, argnums=3), argnums=3)(params, u, t, x)\n",
    "\n",
    "        # res = s_t + s * s_x - 0.01 * s_xx##粘性系数为0.01\n",
    "        res = s_t - s_xx + 1/0.01 * (s**3-s)\n",
    "        return res\n",
    "    \n",
    "        #pou损失\n",
    "    def loss_pou(self, params, batch):\n",
    "        inputs, outputs = batch\n",
    "        u, y = inputs\n",
    "        t, x = y[:,0], y[:,1]\n",
    "        branch_params, trunk_params = params\n",
    "        y = np.stack([t, x], axis=-1)  # 修正这里的维度\n",
    "        T = self.trunk_apply(trunk_params, y)\n",
    "        phi_sum = np.sqrt(np.sum(T**2, axis=-1))\n",
    "        loss = np.mean((phi_sum - 1)**2)\n",
    "        return loss  \n",
    "\n",
    "    # Define initial loss\n",
    "    def loss_ics(self, params, batch):\n",
    "        # Fetch data\n",
    "        inputs, outputs = batch\n",
    "        u, y = inputs\n",
    "\n",
    "        # Compute forward pass\n",
    "        s_pred = vmap(self.operator_net, (None, 0, 0, 0))(params, u, y[:,0], y[:,1])\n",
    "\n",
    "        # Compute loss\n",
    "        loss = np.mean((outputs.flatten() - s_pred)**2)\n",
    "        return loss\n",
    "\n",
    "    # Define boundary loss\n",
    "    def loss_bcs(self, params, batch):\n",
    "        # Fetch data\n",
    "        inputs, outputs = batch\n",
    "        u, y = inputs\n",
    "\n",
    "        # Compute forward pass\n",
    "        s_bc1_pred = vmap(self.operator_net, (None, 0, 0, 0))(params, u, y[:,0], y[:,1])\n",
    "        s_bc2_pred = vmap(self.operator_net, (None, 0, 0, 0))(params, u, y[:,2], y[:,3])\n",
    "\n",
    "        s_x_bc1_pred = vmap(self.s_x_net, (None, 0, 0, 0))(params, u, y[:,0], y[:,1])\n",
    "        s_x_bc2_pred = vmap(self.s_x_net, (None, 0, 0, 0))(params, u, y[:,2], y[:,3])\n",
    "\n",
    "        # Compute loss\n",
    "        loss_s_bc = np.mean((s_bc1_pred - s_bc2_pred)**2)\n",
    "        loss_s_x_bc = np.mean((s_x_bc1_pred - s_x_bc2_pred)**2)\n",
    "\n",
    "        return loss_s_bc + loss_s_x_bc\n",
    "    # Define residual loss\n",
    "    def loss_res(self, params, batch):\n",
    "        # Fetch data\n",
    "        inputs, outputs = batch\n",
    "        u, y = inputs\n",
    "        # Compute forward pass\n",
    "        pred = vmap(self.residual_net, (None, 0, 0, 0))(params, u, y[:,0], y[:,1])\n",
    "\n",
    "        # Compute loss\n",
    "        loss = np.mean((outputs.flatten() - pred)**2)\n",
    "        return loss   \n",
    "\n",
    "    # Define total loss\n",
    "    # 损失函数\n",
    "    def loss(self, params, ics_batch, bcs_batch, res_batch,pou_batch,lambda_pou):\n",
    "        loss_ics = self.loss_ics(params, ics_batch)\n",
    "        loss_bcs = self.loss_bcs(params, bcs_batch)\n",
    "        loss_res = self.loss_res(params, res_batch)\n",
    "        loss_pou = self.loss_pou(params, pou_batch)\n",
    "        loss =  20 * loss_ics +   lambda_pou*loss_pou#修改参数\n",
    "        return loss\n",
    "\n",
    "    # Define a compiled update step\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def step(self, i, opt_state, ics_batch, bcs_batch, res_batch, pou_batch,lambda_pou):\n",
    "        params = self.get_params(opt_state)\n",
    "        g = grad(self.loss)(params, ics_batch, bcs_batch, res_batch, pou_batch,lambda_pou)\n",
    "        return self.opt_update(i, g, opt_state)\n",
    "    # Optimize parameters in a loop\n",
    "    def train(self, ics_dataset, bcs_dataset, res_dataset, pou_dataset,lambda_pou, nIter=10000):\n",
    "        ics_data = iter(ics_dataset)\n",
    "        bcs_data = iter(bcs_dataset)\n",
    "        res_data = iter(res_dataset)\n",
    "        pou_data = iter(pou_dataset)\n",
    "\n",
    "        pbar = trange(nIter)\n",
    "        self.test_loss_log = []\n",
    "        # Main training loop\n",
    "        for it in pbar:\n",
    "            # Fetch data\n",
    "            ics_batch= next(ics_data)\n",
    "            bcs_batch= next(bcs_data)\n",
    "            res_batch = next(res_data)\n",
    "\n",
    "            pou_batch = next(pou_data)\n",
    "            self.opt_state = self.step(next(self.itercount), self.opt_state, ics_batch, bcs_batch, res_batch, pou_batch,lambda_pou)\n",
    "            \n",
    "            if it % 100 == 0:\n",
    "                params = self.get_params(self.opt_state)\n",
    "\n",
    "                # Compute losses\n",
    "                loss_value = self.loss(params, ics_batch, bcs_batch, res_batch, pou_batch,lambda_pou)\n",
    "                loss_ics_value = self.loss_ics(params, ics_batch)\n",
    "                loss_bcs_value = self.loss_bcs(params, bcs_batch)\n",
    "                loss_res_value = self.loss_res(params, res_batch)\n",
    "                test_loss = self.loss(params, test_ics_batch, test_bcs_batch, test_res_batch, test_pou_batch ,lambda_pou)\n",
    "                # Store losses\n",
    "                self.loss_log.append(loss_value)\n",
    "                self.loss_ics_log.append(loss_ics_value)\n",
    "                self.loss_bcs_log.append(loss_bcs_value)\n",
    "                self.loss_res_log.append(loss_res_value)\n",
    "                \n",
    "                self.test_loss_log.append(test_loss)\n",
    "\n",
    "                # Print losses\n",
    "                pbar.set_postfix({'Loss': loss_value, \n",
    "                                'loss_ics' : loss_ics_value,\n",
    "                                'loss_bcs' : loss_bcs_value, \n",
    "                                'loss_physics': loss_res_value})\n",
    "           \n",
    "    # Evaluates predictions at test points  \n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def predict_s(self, params, U_star, Y_star):\n",
    "        s_pred = vmap(self.operator_net, (None, 0, 0, 0))(params, U_star, Y_star[:,0], Y_star[:,1])\n",
    "        return s_pred\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def predict_res(self, params, U_star, Y_star):\n",
    "        r_pred = vmap(self.residual_net, (None, 0, 0, 0))(params, U_star, Y_star[:,0], Y_star[:,1])\n",
    "        return r_pred\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed6b8290-5ef0-4c3b-8423-a3b1c4578e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Geneate ics training data corresponding to one input sample\n",
    "def generate_one_ics_training_data(key, u0, m=101, P=101):# 生成ics初始条件\n",
    "\n",
    "    t_0 = np.zeros((P,1))\n",
    "    x_0 = np.linspace(0, 1, P)[:, None]\n",
    "\n",
    "    y = np.hstack([t_0, x_0])\n",
    "    u = np.tile(u0, (P, 1))\n",
    "    s = u0\n",
    "\n",
    "    return u, y, s\n",
    "#新增\n",
    "def generate_one_pou_training_data(key, m=101, P=100):\n",
    "    # 随机采样时间和空间点\n",
    "    t_pou = random.uniform(key, (P, 1))\n",
    "    x_pou = random.uniform(key, (P, 1))\n",
    "    \n",
    "    y = np.hstack([t_pou, x_pou])  # shape = (P, 2)\n",
    "    u = np.zeros((P, m))           # 不参与pou_loss计算\n",
    "    s = np.zeros((P, 1))           # 不参与pou_loss计算\n",
    "    return u, y, s\n",
    "\n",
    "# Geneate bcs training data corresponding to one input sample\n",
    "def generate_one_bcs_training_data(key, u0, m=101, P=100):\n",
    "\n",
    "    t_bc = random.uniform(key, (P,1))\n",
    "    x_bc1 = np.zeros((P, 1))\n",
    "    x_bc2 = np.ones((P, 1))\n",
    "  \n",
    "    y1 = np.hstack([t_bc, x_bc1])  # shape = (P, 2)\n",
    "    y2 = np.hstack([t_bc, x_bc2])  # shape = (P, 2)\n",
    "\n",
    "    u = np.tile(u0, (P, 1))\n",
    "    y =  np.hstack([y1, y2])  # shape = (P, 4)\n",
    "    s = np.zeros((P, 1))\n",
    "\n",
    "    return u, y, s\n",
    "# Geneate res training data corresponding to one input sample\n",
    "def generate_one_res_training_data(key, u0, m=101, P=1000):\n",
    "\n",
    "    subkeys = random.split(key, 2)\n",
    "   \n",
    "    t_res = random.uniform(subkeys[0], (P,1))\n",
    "    x_res = random.uniform(subkeys[1], (P,1))\n",
    "\n",
    "    u = np.tile(u0, (P, 1))\n",
    "    y =  np.hstack([t_res, x_res])\n",
    "    s = np.zeros((P, 1))\n",
    "\n",
    "    return u, y, s\n",
    "\n",
    "# Geneate test data corresponding to one input sample\n",
    "def generate_one_test_data(idx,usol, m=101, P=101):\n",
    "\n",
    "    u = usol[idx]\n",
    "    u0 = u[0,:]\n",
    "\n",
    "    t = np.linspace(0, 1, P)\n",
    "    x = np.linspace(0, 1, P)\n",
    "    T, X = np.meshgrid(t, x)\n",
    "\n",
    "    s = u.T.flatten()\n",
    "    u = np.tile(u0, (P**2, 1))\n",
    "    y = np.hstack([T.flatten()[:,None], X.flatten()[:,None]])\n",
    "\n",
    "    return u, y, s \n",
    "# Geneate training data corresponding to N input sample\n",
    "def compute_error(idx, usol, m, P):\n",
    "    u_test, y_test, s_test = generate_one_test_data(idx, usol, m, P)\n",
    "\n",
    "    u_test = u_test.reshape(P**2,-1)  \n",
    "    y_test = y_test.reshape(P**2,-1)\n",
    "    s_test = s_test.reshape(P**2,-1)\n",
    "\n",
    "    s_pred = model.predict_s(params, u_test, y_test)[:,None]\n",
    "    error = np.linalg.norm(s_test - s_pred) / np.linalg.norm(s_test) \n",
    "\n",
    "    return error  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "501a70c9-9cd8-4dfc-a57e-4911eccb44e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare the training data\n",
    "\n",
    "# Load data\n",
    "\n",
    "path = 'D:/毕业论文/研究生/code/Allen_cahn/AllenCahn_dataset_1000.mat'  # Please use the matlab script to generate data\n",
    "\n",
    "data = scipy.io.loadmat(path)\n",
    "usol = np.array(data['output'])\n",
    "usol = usol[:,1:1001:10,:]\n",
    "lambda_pou = 0.5\n",
    "# usol = usol[:,1:1001:20,1:100:2] #200*50*50\n",
    "N = usol.shape[0]  # number of total input samples\n",
    "N_train = 100      # number of input samples used for training#修改参数\n",
    "N_test = 10 # number of input samples used for test\n",
    "m = 100           # number of sensors for input samples\n",
    "\n",
    "# P_ics_train = 100\n",
    "P_ics_train = 100   # number of locations for evulating the initial condition\n",
    "P_bcs_train = 100    # number of locations for evulating the boundary condition\n",
    "P_res_train = 2500   # number of locations for evulating the PDE residual\n",
    "P_test = 100        # resolution of uniform grid for the test data\n",
    "# m = 50\n",
    "# P_test = 50\n",
    "# P_ics_train = 50   # number of locations for evulating the initial condition\n",
    "# P_bcs_train = 50    # number of locations for evulating the boundary condition\n",
    "# P_res_train = 2500   # number of locations for evulating the PDE residual\n",
    "\n",
    "u0_train = usol[:N_train,0,:]   # input samples\n",
    "# usol_train = usol[:N_train,:,:]\n",
    "\n",
    "key = random.PRNGKey(0) # use different key for generating test data \n",
    "keys = random.split(key, N_train)\n",
    "\n",
    "# Generate training data for inital condition\n",
    "u_ics_train, y_ics_train, s_ics_train = vmap(generate_one_ics_training_data, in_axes=(0, 0, None, None))(keys, u0_train, m, P_ics_train)\n",
    "\n",
    "u_ics_train = u_ics_train.reshape(N_train * P_ics_train,-1)  \n",
    "y_ics_train = y_ics_train.reshape(N_train * P_ics_train,-1)\n",
    "s_ics_train = s_ics_train.reshape(N_train * P_ics_train,-1)\n",
    "\n",
    "# Generate training data for boundary condition\n",
    "u_bcs_train, y_bcs_train, s_bcs_train = vmap(generate_one_bcs_training_data, in_axes=(0, 0, None, None))(keys, u0_train, m, P_bcs_train)\n",
    "\n",
    "u_bcs_train = u_bcs_train.reshape(N_train * P_bcs_train,-1)  \n",
    "y_bcs_train = y_bcs_train.reshape(N_train * P_bcs_train,-1)\n",
    "s_bcs_train = s_bcs_train.reshape(N_train * P_bcs_train,-1)\n",
    "\n",
    "# Generate training data for PDE residual\n",
    "u_res_train, y_res_train, s_res_train = vmap(generate_one_res_training_data, in_axes=(0, 0, None, None))(keys, u0_train, m, P_res_train)\n",
    "\n",
    "u_res_train = u_res_train.reshape(N_train * P_res_train,-1)\n",
    "y_res_train = y_res_train.reshape(N_train * P_res_train,-1)\n",
    "s_res_train = s_res_train.reshape(N_train * P_res_train,-1)\n",
    "\n",
    "#新增pou\n",
    "u_pou_train, y_pou_train, s_pou_train = vmap(generate_one_pou_training_data, in_axes=(0, None, None))(keys, m, P_bcs_train)\n",
    "u_pou_train = u_pou_train.reshape(N_train * P_bcs_train,-1)\n",
    "y_pou_train = y_pou_train.reshape(N_train * P_bcs_train,-1)\n",
    "s_pou_train = s_pou_train.reshape(N_train * P_bcs_train,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72e23c17-ae04-4fda-bdc2-d2d365cce29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "branch_layers = [m, 100, 100, 100, 100, 100, 100, 100]\n",
    "trunk_layers =  [2, 100, 100, 100, 100, 100, 100, 100]\n",
    "model = PI_DeepONet(branch_layers, trunk_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6eeb7b59-1467-4c98-825e-6f5e0665b8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data set\n",
    "batch_size = 50\n",
    "# batch_size = 10\n",
    "ics_dataset = DataGenerator(u_ics_train, y_ics_train, s_ics_train, batch_size)\n",
    "bcs_dataset = DataGenerator(u_bcs_train, y_bcs_train, s_bcs_train, batch_size)\n",
    "res_dataset = DataGenerator(u_res_train, y_res_train, s_res_train, batch_size)\n",
    "pou_dataset = DataGenerator(u_pou_train, y_pou_train, s_pou_train, batch_size)\n",
    "# 训练前加上这段\n",
    "# test_batch_size = 10  # 或你想要的测试batch大小\n",
    "test_batch_size = 50\n",
    "test_ics_dataset = DataGenerator(u_ics_train, y_ics_train, s_ics_train, test_batch_size)\n",
    "test_bcs_dataset = DataGenerator(u_bcs_train, y_bcs_train, s_bcs_train, test_batch_size)\n",
    "test_res_dataset = DataGenerator(u_res_train, y_res_train, s_res_train, test_batch_size)\n",
    "test_pou_dataset = DataGenerator(u_pou_train, y_pou_train, s_pou_train, test_batch_size)\n",
    "\n",
    "test_ics_batch = next(iter(test_ics_dataset))\n",
    "test_bcs_batch = next(iter(test_bcs_dataset))\n",
    "test_res_batch = next(iter(test_res_dataset))\n",
    "test_pou_batch = next(iter(test_pou_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77c82108-18c4-44c7-b1b1-aa40c5ddd594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/3000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Bad StatusOr access: RESOURCE_EXHAUSTED: Failed to allocate buffer for Literal",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Note: may meet OOM issue if use Colab. Please train this model on the server.  \u001b[39;00m\n\u001b[0;32m      3\u001b[0m nIter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3000\u001b[39m\n\u001b[1;32m----> 4\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(ics_dataset, bcs_dataset, res_dataset, pou_dataset,lambda_pou,nIter\u001b[38;5;241m=\u001b[39mnIter)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# # Restore the trained model\u001b[39;00m\n\u001b[0;32m      7\u001b[0m params \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_params(model\u001b[38;5;241m.\u001b[39mopt_state)\n",
      "Cell \u001b[1;32mIn[19], line 143\u001b[0m, in \u001b[0;36mPI_DeepONet.train\u001b[1;34m(self, ics_dataset, bcs_dataset, res_dataset, pou_dataset, lambda_pou, nIter)\u001b[0m\n\u001b[0;32m    141\u001b[0m ics_batch\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(ics_data)\n\u001b[0;32m    142\u001b[0m bcs_batch\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(bcs_data)\n\u001b[1;32m--> 143\u001b[0m res_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(res_data)\n\u001b[0;32m    145\u001b[0m pou_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(pou_data)\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitercount), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_state, ics_batch, bcs_batch, res_batch, pou_batch,lambda_pou)\n",
      "Cell \u001b[1;32mIn[18], line 16\u001b[0m, in \u001b[0;36mDataGenerator.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGenerate one batch of data\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey, subkey \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey)\n\u001b[1;32m---> 16\u001b[0m inputs, outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__data_generation(subkey)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inputs, outputs\n",
      "    \u001b[1;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\jax\\_src\\compiler.py:378\u001b[0m, in \u001b[0;36mbackend_compile_and_load\u001b[1;34m(backend, module, executable_devices, options, host_callbacks)\u001b[0m\n\u001b[0;32m    369\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcompile_and_load(\n\u001b[0;32m    370\u001b[0m           built_c,\n\u001b[0;32m    371\u001b[0m           executable_devices\u001b[38;5;241m=\u001b[39mexecutable_devices,\n\u001b[0;32m    372\u001b[0m           compile_options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    373\u001b[0m           host_callbacks\u001b[38;5;241m=\u001b[39mhost_callbacks,\n\u001b[0;32m    374\u001b[0m       )\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n\u001b[1;32m--> 378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcompile_and_load(\n\u001b[0;32m    379\u001b[0m         built_c,\n\u001b[0;32m    380\u001b[0m         executable_devices\u001b[38;5;241m=\u001b[39mexecutable_devices,\n\u001b[0;32m    381\u001b[0m         compile_options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    382\u001b[0m     )\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m xc\u001b[38;5;241m.\u001b[39mXlaRuntimeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    384\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m error_handler \u001b[38;5;129;01min\u001b[39;00m _XLA_RUNTIME_ERROR_HANDLERS:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Bad StatusOr access: RESOURCE_EXHAUSTED: Failed to allocate buffer for Literal"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "# Note: may meet OOM issue if use Colab. Please train this model on the server.  \n",
    "nIter = 3000\n",
    "model.train(ics_dataset, bcs_dataset, res_dataset, pou_dataset,lambda_pou,nIter=nIter)\n",
    "\n",
    "# # Restore the trained model\n",
    "params = model.get_params(model.opt_state)\n",
    "\n",
    "lam = f\"{N_train}_{N_test}_{nIter}_pou\"\n",
    "flat_params, _ = ravel_pytree(params)\n",
    "# np.save('modified_MLP_lam_{}_params_{}.npy'.format(lam,lambda_pou), flat_params)\n",
    "# print(\"Parameters saved to 'modified_MLP_lam_{}_params_{}.npy'\".format(lam, lambda_pou))\n",
    "# ...existing code...\n",
    "\n",
    "# params = model.unravel_params(np.load('modified_MLP_lam_{}_params_{}.npy'.format(lam, lambda_pou)))\n",
    "# Compute relative l2 error over test data\n",
    "idx = random.randint(key=random.PRNGKey(12345), shape=(400,), minval=N_train, maxval=2000)\n",
    "# k= 100\n",
    "k = N_train\n",
    "# N_test = 100\n",
    "idx = np.arange(k, k + N_test)\n",
    "\n",
    "errors = vmap(compute_error, in_axes=(0, None, None, None))(idx, usol, m, P_test)\n",
    "mean_error = errors.mean()\n",
    "\n",
    "print('Mean relative L2 error of s: {:.2e}'.format(mean_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425b4b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{N_train}_{N_test}_pou\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a23a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.loss_ics_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bea9d1-f7b2-4ab3-8f4c-7f363d867909",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.save('loss_log_{}_{}_pou.npy'.format(lam,lambda_pou),np.array(model.loss_log))\n",
    "# 保存测试损失日志\n",
    "np.save('test_loss_log_{}_{}_pou.npy'.format(lam,lambda_pou), np.array(model.test_loss_log))\n",
    "loss_log = np.load('loss_log_{}_{}_pou.npy'.format(lam,lambda_pou))\n",
    "test_loss_log = np.load('test_loss_log_{}_{}_pou.npy'.format(lam,lambda_pou))\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(model.loss_log, lw=2, label='Train Loss')\n",
    "plt.plot(model.test_loss_log, lw=2, label='Test Loss')\n",
    "plt.xlabel('Iteration (x100)')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a96563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "errors = vmap(compute_error, in_axes=(0, None, None, None))(idx, usol, m, P_test)\n",
    "mean_error = errors.mean()\n",
    "\n",
    "print('Mean relative L2 error of s: {:.2e}'.format(mean_error))\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80560d19-0152-4dad-a9dc-6d62e004f4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for one generated data\n",
    "k = 3 # index\n",
    "u = usol[k,:, :]\n",
    "u0 = usol[k,0,:]\n",
    "\n",
    "P_test = 100\n",
    "\n",
    "t = np.linspace(0, 1, P_test)\n",
    "x = np.linspace(0, 1, P_test)\n",
    "T, X = np.meshgrid(t, x)\n",
    "\n",
    "u_test = np.tile(u0, (P_test**2, 1))\n",
    "y_test = np.hstack([T.flatten()[:,None], X.flatten()[:,None]])\n",
    "s_test = u.flatten()[:,None]\n",
    "\n",
    "s_pred = model.predict_s(params, u_test, y_test)[:,None]\n",
    "S_pred = griddata(y_test, s_pred.flatten(), (T, X), method='cubic')\n",
    "\n",
    "\n",
    "error_s = np.linalg.norm(u - S_pred.T, 2) / np.linalg.norm(u, 2) \n",
    "\n",
    "print(\"error_s: {:.3e}\".format(error_s))\n",
    "\n",
    "print(\"error_s: {:.3e}\".format(error_s))\n",
    "# 保存第一个子图 - 精确解\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.pcolor(T, X, u, cmap='jet')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('t')\n",
    "plt.title('Exact')\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f\"{save_path}l2_exact_k_{k}.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 保存第二个子图 - 预测值\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.pcolor(T, X, S_pred.T, cmap='jet')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('t')\n",
    "plt.title('Prediction')\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f\"{save_path}l2_k_pred{k}.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 保存第三个子图 - 绝对误差\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.pcolor(T, X, np.abs(S_pred.T - u), cmap='jet')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('t')\n",
    "plt.title('Absolute error')\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f\"{save_path}l2_k_error{k}.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 创建并显示完整图形（包含所有三个子图）\n",
    "fig = plt.figure(figsize=(18,5))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.pcolor(T, X, u, cmap='jet')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('t')\n",
    "plt.title('Exact')\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.pcolor(T, X, S_pred.T, cmap='jet')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('t')\n",
    "plt.title('Prediction')\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.pcolor(T, X, np.abs(S_pred.T - u), cmap='jet')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('t')\n",
    "plt.title('Absolute error')\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d81d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "error_s = np.linalg.norm(u - S_pred.T, 2) / np.linalg.norm(u, 2)\n",
    "error_percent = error_s * 100\n",
    "print(\"Relative L2 error: {:.2f}%\".format(error_percent))\n",
    "# ...existing code..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
